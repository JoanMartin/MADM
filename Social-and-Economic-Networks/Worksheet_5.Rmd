---
title: "Social and economic networks"
subtitle: "Handout 5"
author: "Juan José Martín y Christian Strasser"
output:
  html_document:
    number_sections: false
    highlight: tango
    toc: yes
    df_print: paged
editor_options: 
  chunk_output_type: console
---



<style type="text/css">

  body {
    background-color: #f6f7fd; 
  }
  
  a:link {
    color: #0174DF;
  }
  
  code.r {
    font-size: 14px;
  } 
  
  div pre {
    background-color:#E0ECF8;
  }
  pre {
    font-size: 14px 
  }
 
  p {
    text-align: justify;
  }
 
    
  h1, h2, h3, h4, h5, h6 {
    color: #737aaa;
  }

  th {  
    background-color:#737aaa;
    color: #FAFAFA;
    padding:5px;
  }
  
  td {
    font-size: 11.5pt;
  } 
  
  tr:nth-child(even){
    background: white;
  }
  
  tr:nth-child(odd){ 
    background-color: #EFF8FB;
  }
</style>

***  


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=8, fig.align="center", echo=TRUE, warning=FALSE, message=FALSE)
library(centiserve)
library(igraph)
```


**1)** It has been observed in many networks an association between "centrality" and "lethality," defined as the fatal disconnection of the network when nodes are removed. Let's study this association on an undirected network describing an old map of the Internet at the autonomous system level. The links in this network are contained in the file **AS-19971108.dat**. Download it on your computer and upload it to R as a dataframe. Define an undirected graph with this list of edges.

```{r}
df = read.csv("datasets/AS-19971108.dat", sep = " ")
g = graph_from_data_frame(df, directed = FALSE)
```

<br>

*a)* Compute its basic indices: order, size, density, number of connected components, diameter, transitivity.

Orden:

```{r}
gorder(g)
```

<br>

Tamaño:

```{r}
gsize(g)
```

<br>

Densidad:

```{r}
edge_density(g, loops = FALSE)
```

<br>

Componentes conectadas:

```{r}
length(decompose(g))
```

<br>

Diámetro:

```{r}
diameter(g, directed = FALSE)
```

<br>

Transitividad:

```{r}
round(transitivity(g),4)
```

<br>

*b)* Repeat 1000 times the procedure of removing a random 0.1% of its set of nodes, and compute the average number of connected components of the resulting networks and the average fraction of the network represented by the largest component. (The function **replicate** is useful to avoid for-loops in, well, replications.)

```{r}
Avg_CC = c()
Avg_LC = c()
r = round(gorder(g) * 0.001)
set.seed(100)

for(i in 1:1000) {
  n_random = sample(1:gorder(g), r, replace = FALSE)
  g_new = delete.vertices(g, n_random)
  comp = components(g_new)
  Avg_CC = c(Avg_CC, comp$no)
  Avg_LC = c(Avg_LC, max(comp$csize) / gorder(g_new))
}

mean(Avg_CC)
mean(Avg_LC)
```

```{r}
set.seed(100)

foo = function() {
  n_random = sample(1:gorder(g), r, replace = FALSE)
  g_new = delete.vertices(g, n_random)
  comp = components(g_new)
  n_comp = comp$no
  avg_f = max(comp$csize) / gorder(g_new)
  return (c(n_comp, avg_f))
}

Avg = replicate(n = 1000, foo())
mean(Avg[1,])
mean(Avg[2,])
```

<br>

*c)* Now, compute the number of connected components and the fraction represented by the largest component of the networks obtained after removing the most central 0.1% of nodes, for the following centrality indices (of course, if the most central 0.1% of nodes for two indices are the same set of nodes, you need not waste your time considering twice the same network): 

* degree

```{r}
r = round(gorder(g) * 0.001)
g_new = delete.vertices(g, names(round(igraph::degree(g,normalized=TRUE),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* connectedness

```{r}
r = round(gorder(g) * 0.001)
g_new = delete.vertices(g, names(round(sort(1/eccentricity(g),decreasing =TRUE),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* decay with $\delta=0.5$

```{r}
r = round(gorder(g) * 0.001)
g_new = delete.vertices(g, names(round(sort(decay(g, decay=0.5),decreasing=TRUE),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* betweenness

```{r}
r = round(gorder(g) * 0.001)
g_new = delete.vertices(g, names(round(igraph::betweenness(g),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* eigenvector

```{r}
r = round(gorder(g) * 0.001)
g_new = delete.vertices(g, names(round(eigen_centrality(g,scale=FALSE)$vector,4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* $\alpha$-centrality with $\alpha$ around half its maximum sensible value (use the functions of the package **rARPACK** to compute fast the first eigenvalue). 

```{r}
r = round(gorder(g) * 0.001)
g_new = delete.vertices(g, names(sort(alpha_centrality(g,alpha=0.5), decreasing=TRUE))[1:r])
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

*d)* Repeat the last two points replacing 0.1% by 1%.

* degree

```{r}
r = round(gorder(g) * 0.01)
g_new = delete.vertices(g, names(round(igraph::degree(g,normalized=TRUE),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* connectedness

```{r}
r = round(gorder(g) * 0.01)
g_new = delete.vertices(g, names(round(sort(1/eccentricity(g),decreasing =TRUE),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* decay with $\delta=0.5$

```{r}
r = round(gorder(g) * 0.01)
g_new = delete.vertices(g, names(round(sort(decay(g, decay=0.5),decreasing=TRUE),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* betweenness

```{r}
r = round(gorder(g) * 0.01)
g_new = delete.vertices(g, names(round(igraph::betweenness(g),4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* eigenvector

```{r}
r = round(gorder(g) * 0.01)
g_new = delete.vertices(g, names(round(eigen_centrality(g,scale=FALSE)$vector,4)[1:r]))
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

<br>

* $\alpha$-centrality with $\alpha$ around half its maximum sensible value (use the functions of the package **rARPACK** to compute fast the first eigenvalue).

```{r}
r = round(gorder(g) * 0.01)
g_new = delete.vertices(g, names(sort(alpha_centrality(g,alpha=0.5), decreasing=TRUE))[1:r])
comp = components(g_new)
comp$no
max(comp$csize) / gorder(g_new)
```

*e)* Comment the results obtained.

Todos los valores obtenidos son el resultado de eliminar una cantidad de nodos "centrales" según un tipo de centralidad u otra. Después de la eliminación, analizamos el número de componendes desconectadas que obtenemos y cuanto mayor es este valor, mayor es la importancia de estos nodos eliminados. Su "centralidad" es elevada.

**2)** What 0.5% of the nodes do you consider more "central" in the Facebook network studied in Handout 4? What kind of centrality (or centralities) do you use and why? Do, for your centrality index or indices, the corresponding 0.5% most central nodes contain good candidates to be the users around which the network was built? What candidates?

```{r}
dff = read.csv("datasets/facebook_sample_anon.txt", sep = " ")
gf = graph_from_data_frame(dff, directed = FALSE)
```

Connectedness
```{r}
r = round(gorder(gf) * 0.005)
names(round(sort(1/eccentricity(gf),decreasing =TRUE),4)[1:r])
```
Son los nodos más "centrales" considerando su nivel de _connectedness_, ya que una vez eliminados estos nodos, obtenemos el mayor número de componentes desconectadas de la red de Facebook.

**3)** What fraction of individuals in the Facebook network studied in Handout 4 have a number of friends smaller than the mean number of friends of their friends? And what fraction of individuals in that network have less friends than every friend of theirs?


