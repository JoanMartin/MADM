---
title: "Hoja de Ejercicios 3"
subtitle: '**Aprendizaje Estadístico y Toma de Decisiones.**'
author: "Juan José Martín, Marina Moreno, Christian Strasser, Maria del Mar Bibiloni."
output:
  html_document:
    number_sections: false
    highlight: tango
    toc: yes
    df_print: paged
editor_options: 
  chunk_output_type: console
---



<style type="text/css">

  body {
    background-color: #f6f7fd; 
  }
  
  a:link {
    color: #0174DF;
  }
  
  code.r {
    font-size: 14px;
  } 
  
  div pre {
    background-color:#E0ECF8;
  }
  pre {
    font-size: 14px 
  }
 
  p {
    text-align: justify;
  }
 
    
  h1, h2, h3, h4, h5, h6 {
    color: #737aaa;
  }

  th {  
    background-color:#737aaa;
    color: #FAFAFA;
    padding:5px;
  }
  
  td {
    font-size: 11.5pt;
  } 
  
  tr:nth-child(even){
    background: white;
  }
  
  tr:nth-child(odd){ 
    background-color: #EFF8FB;
  }
</style>

***       


```{r}
library(stats)
library(ggplot2)
```

#Ejercicio 9

**Nos interesa estimar una regresión logística en la que queremos explicar una variable binomial (0,1) con una variable $X$. Por el problema de endogenidad, lo estimamos en 2 etapas. En la primera etapa en vez de regresar $Y$ sobre $X$, primero se hace una regresión de $X$ sobre cualquier combinación de instrumentos posibles $\{Z_1,Z_2,Z_3\}$ y se guarda el valor ajustado $\tilde{X}$, después se hace una regresión de la variable continua $Y$ sobre  $\tilde{X}$. **

El método de las variables instrumentales se utiliza cuando puede existir correlación entre la variable independiente y el término de error. Para evitar que esto suceda,  se busca una/s variable/s instrumental/es $Z$ de manera que $Corr(Y,Z)=0$ pero que $Corr(X,Z)\neq 0$. 

El método se basa en buscar cuál es la mejor regresión ($\tilde{X}$) de la variable independiente $X$ con las variables instrumentales $Z_i$ de manera que el error en la regresión $Y$ sobre $\tilde{X}$ sea mínimo. Para saber con qué variables instrumentales es mejor realizar la regresión sobre $X$ utilizaremos el método de validación cruzada con dos etapas.

<br>

(a) **Explica cómo se utilizaría la validación cruzada para evaluar que combinación de instrumentos da los mejores resultados. **

El problema a resolver es el siguiente: 

$$\tilde{X} = \alpha Z_1 +\beta Z_2 +\gamma Z_3 $$

Vamos a seleccionar con validación cruzada cuál es la mejor combinación de variables instrumentales para predecir $Y$ sobre $\tilde{X}$, es decir, qué coeficientes $\alpha, \beta, \gamma$ igualamos a 1 o anulamos para obtener una predicción final mejor. Notemos que tenemos $2^3=8$ combinaciones para anular o mantener los coeficentes anteriores y en consecuencia tenemos 8 modelos posibles.

Se realizaran 5 (o 10) particiones de los datos, y en cada modelo se realiazará la regresión $Y$ sobre $\tilde{X}$ (que estemos considerando) con 4 de los 5 trozos de los datos y con el quinto trozo se calculará el error de predicción. Manteniendo la misma partición, se volverá a calcular la regresión $Y$ sobre $\tilde{X}$, pero esta vez cambiando el trozo de validación. En consecuencia tendremos 5 errores de predicción para cada modelo de $\tilde{X}$, uno para cada trozo de la partición que hemos dejado para calcular el error de predicción. Se suman estos 5 errores y así obtenemos el error de predicción del modelo con el que estabamos trabajando.

Este mismo proceso se realiza con los otros siete modelos posibles restantes y nos quedamos con el que obtengamos un error de predicción mínimo.

<br>

(b) **Explica cómo se utilizaría el bootstrap para calcular la desviación estándar del coeficiente de regresión de $Y$ sobre $X$.**

Una vez tenemos el modelo de regresión $Y = \alpha X$ , queremos saber cómo es de fiable el parámetro $\alpha$ que hemos estimado. Para ello cogeremos 1000 muestras aleatorias simples de los datos de los que disponemos con reemplazo y calculamos para cada m.a.s. el coeficiente de la regresión. Una vez tengamos estimados el coeficiente $\alpha$ para cada regresión calculamos su desviación típica del vector de coeficientes. De este modo obtendremos la desviación estándar del coeficiente de regresión de $Y$ sobre $X$.

<br>


#Ejercicio 10

**Dibuja el resultado de una predicción de 2 vecinos más cercanos de una regresión de los puntos en un examen sobre horas estudiadas. ¿Cómo te parece la elección de $K=2$?**

<br>


#Ejercicio 11

**Analizamos 3 variables con la matriz de correlaciÛn dada por**

(a) **¿Tiene sentido hacer un análisis de componentes principales?**

No

<br>

(b) **¿Cuántos componentes principales crees que encontramos (elegimos)?**

3

<br>

(c) **¿Qué porcentaje de la varianza total explica aproximadamente cada componente principal?** 

1/3

<br>


#Ejercicio 13

**Te gustaría invertir dinero en acciones. Algún amigo comenta que acciones de tecnologia han crecido los últimos años más rápidamente. Sin embargo quieres basar tu decisión en datos y comparas los rendimientos de acciones de tecnología con los demás. Encuentras que la diferencia es positiva y altamente signicativa. Por eso inviertes. Te parece bien el análisis?**

En primer lugar, debemos comentar que el estudio que quiere realizar la persona que quiere invertir es insuficiente y elemental. Nosotros propondríamos un estudio basado en buscar correlaciones positivas y negativas en diferentes sectores y que estos sectores fueran más específicos (y no: o tecnnolgia o no tecnologia). Además en el supuesto del eunciado no se contempla el hecho de que se pueda producir un suceso externo que provoque la caída del precio de las acciones.

En segundo lugar, el sujeto del enunciado que quiere invertir peca en una de las pautas básicas para la creación de cualquier modelo. Se supone que el individuo realiza un modelo con datos históricos pero en ningún momento se verifica o contrasta con datos nuevos para obtenerun error de predicción y saber cómo de bueno es el modelo.

En conclusión, el análisis que se realiza no nos parece bien, ya que es necesario la obtención de nuevos datos para poder dar el modelo como válido.

<br>


#Ejercicio 14

**Algunos de los resultados de un análisis de componentes principales se muestran en las siguientes tablas. Valore e interprete los resultados obtenidos.**

En primer lugar, observemos que todas las variables estan medidas sobre una misma unidad de medida, en consecuencia no seria necesario escalar los datos con desviación estandar a 1, aunque se puede realizar.

En segundo lugar, en la tabla de los 4 estadísticos principales podemos observar que la mayoria de variables presentan una desviación típica grande, en consequencia podemos tener problemas a la hora de encontrar una reducción de la dimensionalidad porque tenemos un conjunto de datos bastante disperso.

En tercer lugar, en la matriz de correlaciones se puede observar que no existe una gran correlación entre las variables. El mayor valor se presenta entre la variable *PMIN* y *SMEAN* y es de 0.832. El siguiente valor más grande es de 0.6951 entre la variable *PMEAN* y *PMIN*. Este hecho también nos indica que vamos a tener dificultades para encontrar pocas componentes principales que expliquen la mayoria de datos, ya que cuanta más correlación hay entre las variables, más se explica una en función de la otra y esto permite reducir la dimensión del problema con una componente principal combinación lineal de ambas.

En cuarto lugar podemos observar, que el problema se ha intentado resolver con dos componentes principales, de las cuales la primera explica un 0.5352 de la variabilidad de los datos y la segunda un 0.1993 de los datos, en consecuencia, entre las dos explican un total del 0.7344 de la variabilidad. 

Finalmente, podemos observar las cargas de las dos componentes en la tabla *Eigenvectors (CORR)*. Si nos fijamos en dicha tabla, obtenemos que la primera componente pincipal da una información bastante homogénia sobre todas las variables, por tanto podemos afirmar que la primera componente principal nos da información sobre la contaminación en general. Por otro lado, en la segunda componente principal, podemos diferenciar que hay una correlación positiva entre las variables que dan información sobre sulfatos y una correlación negativa sobre las variables que dan información sobre partículas, en consecuencia con esta componente principal obtenemos información sobre el tipo de contaminación.

<br>


#Ejercicio 15

**Utiliza los datos, en el fichero "satisfacción" y haz un análisis de componentes principales.**

```{r, include=FALSE}
load("datasets/satisfaccion.RData")
```

El fichero *satisfaccion* contiene el nivel de satisfacción de varios turistas que realizaron sus vacaciones en Baleares en agosto de 2003. Este nivel, respecto las siguientes características (variables), se mide de 1 (pésimo) a 10 (excelente).

* _PAISAJE_: nivel de satisfacción respecto al paisaje.
* _PLAYA_: nivel de satisfacción respecto a la playa.
* _CLIMA_: nivel de satisfacción respecto al clima.
* _CALALOJ_: nivel de satisfacción respecto a la calidad del alojamiento.
* _CALMEDIO_: nivel de satisfacción respecto a la calidad medioambiental.
* _CALURBAN_: nivel de satisfacción respecto a la calidad del entorno urbano.
* _LIMPIEZA_: nivel de satisfacción respecto a la limpieza.
* _PRCOMID_: nivel de satisfacción respecto al precio de las comidas.
* _PROCIO_: nivel de satisfacción respecto al precio de las actividades de ocio.
* _PRCOMPRA_: nivel de satisfacción respecto al precio de compras comerciales.
* _TRATO_: nivel de satisfacción respecto al trato recibido como cliente.
* _HOSPIT_: nivel de satisfacción respecto a la hospitalidad de la gente.
* _SEGUR_: nivel de satisfacción respecto a la seguridad.
* _DIVERS_: nivel de satisfacción respecto a la diversión nocturna.
* _INFORM_: nivel de satisfacción respecto a la información.
* _SEÑAL_: nivel de satisfacción respecto a la señalización.
* _CULTURA_: nivel de satisfacción respecto a actividades y atractivos culturales.
* _TRANQUIL_: nivel de satisfacción respecto a la tranquilidad.
* _RUIDO_: nivel de satisfacción respecto al ruido.

<br>

Veamos la satisfacción de algunos turistas.

```{r}
pcegt_satisfaccion=as.data.frame(pcegt_satisfaccion)
head(pcegt_satisfaccion)
```

<br>

Como podemos observar, hay algunas filas vacías, por lo que las quitamos para hacer el análissi de componentes principales. 

```{r}
satisfaccion=pcegt_satisfaccion[complete.cases(pcegt_satisfaccion), ]
```

<br>

Totas las caractarísticas tienen las mismas unidades, _nivel de satisfacción de 1 a 10_, por tanto, parece que no hace falta estandarizar los datos. Veamos la varianza de las variables.

```{r}
apply(satisfaccion, 2, var)
```

<br>

Dado que la diferencia entre las varianzas no es muy grande, no estandarizaremos los datos.

Calculemos las componentes principales.

```{r}
pc_satisfaccion=prcomp(satisfaccion, scale=TRUE)
ncol(pc_satisfaccion$rotation)
```

<br>

Tenemos 28 componentes principales.

Ahora, visualizamos parte de la matriz de vectores de carga de los componentes principales.

```{r}
head(as.data.frame(pc_satisfaccion$rotation))
```

<br>

Con la función _Summary_, podemos ver la proporción de varianza explicada y la proporción de varianza acumulada.

```{r}
sum_pc=summary(pc_satisfaccion)
sum_pc
```

<br>

La proporcion de varianza explicada es poca para los primeros componentes. Veámoslo en un gràfico.

```{r, fig.align='center'}
plot(1:28,sum_pc$importance[2,]  , xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,0.3), pch=21, type='b',bg="cyan", col="blue" )
```

<br>

Aunque no es suficiente para explicar los datos, vemos como el primer compoente es el que más explica con diferencia.

Ahora, la idea es usar la proporción de varianza explicada acumulada para ver que componentes son suficientes para explicar la satisfacción de los turistas. Así, tomaremos las componentes que expliquen más de un $80\%$ de los datos. Visualicemos las proporciones.

```{r, fig.align='center'}
plot(1:28,sum_pc$importance[3,], xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), pch=21, type='b',bg="mediumspringgreen", col="green4" )
abline(h=0.8, col="red", lty="dashed")
```

<br>

Parece que podriamos reducir la dimensión hasta 14 componentes principales. Comprobémoslo.

```{r}
which(sum_pc$importance[3,]>=0.8)[1]
```

<br>

El valor de proporción aculada para esta componente 14 es 0.80286.

Aunque no podemos dibujar las 14 componentes principales, sí podemos representar las dos primeras tal como sigue.

```{r}
PCbiplot = function(PC, x="PC1", y="PC2", colors=c('black', 'black', 'red', 'red')) {
        data = data.frame(obsnames=row.names(PC$x), PC$x)
        
        plot = ggplot(data, aes_string(x=x, y=y)) + 
          geom_point(alpha = 0.25, col="turquoise2")
        
        plot = plot + 
          geom_hline(yintercept = 0,aes(0), size=.2) + 
          geom_vline(aes(0), size=.2, color=colors[2], xintercept = 0)
        
        datapc = data.frame(varnames=rownames(PC$rotation), PC$rotation)
        
        mult = min(
                (max(data[,y]) - min(data[,y])/(max(datapc[,y])-min(datapc[,y]))),
                (max(data[,x]) - min(data[,x])/(max(datapc[,x])-min(datapc[,x])))
        )
        datapc = transform(datapc,
                            v1 = .7 * mult * (get(x)),
                            v2 = .7 * mult * (get(y))
        )
        
        plot = plot + 
          coord_equal() + 
          geom_text(data=datapc, aes(x=v1, y=v2, label=varnames), 
                    size = 4, vjust=1, hjust=1.25, color=colors[3])
        
        plot = plot + 
          geom_segment(data=datapc, aes(x=0, y=0, xend=v1, yend=v2), 
                       arrow=arrow(length=unit(0.2,"cm")), alpha=0.75, color=colors[4])
        
        plot
}
PCbiplot(pc_satisfaccion, colors=c("black", "black", "red2", "tan1"))
```


