{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Not so) Big Data\n",
    "---\n",
    "En el siguiente enlace encontrarás un fichero de datos comprimido. [here](https://drive.google.com/open?id=1Kr8k8tmN2ziskPwLW_A8lQ_M2-5vHKsa)\n",
    "\n",
    "Descargalo y descomprimelo en tu local. Una vez hecho esto, realiza los siguientes ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Carga una sesión de spark local. Comprueba que el UI de Spark está activo en [localhost:4040](localhost:4040)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, mean, round\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Crea un dataframe de Spark y carga el fichero. **\n",
    "\n",
    "*Nota: está separado por tabulador*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_adds = spark.read \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"header\", \"false\") \\\n",
    "            .option(\"delimiter\", \"\\t\") \\\n",
    "            .csv('/home/jovyan/MADM/opt/adds.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Mostramos el número de filas totales que tiene el dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45840617"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_adds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Y su esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_adds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Muestra la primera fila. ¿Cuántas variables numéricas hay?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----+---+---+---+---+----+----+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----+--------+--------+--------+--------+\n",
      "|_c0|_c1|_c2|_c3|_c4| _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|    _c14|    _c15|    _c16|    _c17|    _c18|    _c19|    _c20|    _c21|    _c22|    _c23|    _c24|    _c25|    _c26|    _c27|    _c28|    _c29|    _c30|    _c31|    _c32|    _c33|    _c34|_c35|    _c36|    _c37|    _c38|    _c39|\n",
      "+---+---+---+---+---+----+---+---+---+---+----+----+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----+--------+--------+--------+--------+\n",
      "|  0|  1|  1|  5|  0|1382|  4| 15|  2|181|   1|   2|null|   2|68fd1e64|80e26c9b|fb936136|7b4723c4|25c83c98|7e0ccccf|de7995b8|1f89b562|a73ee510|a8cd5504|b2cb9c98|37c9c164|2824a5f6|1adce6ef|8ba8b39a|891b62e7|e5ba7672|f54016b9|21ddcdc9|b1252a9d|07b5194c|null|3a171ecb|c5c50484|e8b83407|9727dd16|\n",
      "+---+---+---+---+---+----+---+---+---+---+----+----+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----+--------+--------+--------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_adds.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. Crea un dataframe de spark sólo con las variables numéricas `dfs_num` **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuáles son los tipos de todas las variables del dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('_c1', 'int'),\n",
       " ('_c2', 'int'),\n",
       " ('_c3', 'int'),\n",
       " ('_c4', 'int'),\n",
       " ('_c5', 'int'),\n",
       " ('_c6', 'int'),\n",
       " ('_c7', 'int'),\n",
       " ('_c8', 'int'),\n",
       " ('_c9', 'int'),\n",
       " ('_c10', 'int'),\n",
       " ('_c11', 'int'),\n",
       " ('_c12', 'int'),\n",
       " ('_c13', 'int'),\n",
       " ('_c14', 'string'),\n",
       " ('_c15', 'string'),\n",
       " ('_c16', 'string'),\n",
       " ('_c17', 'string'),\n",
       " ('_c18', 'string'),\n",
       " ('_c19', 'string'),\n",
       " ('_c20', 'string'),\n",
       " ('_c21', 'string'),\n",
       " ('_c22', 'string'),\n",
       " ('_c23', 'string'),\n",
       " ('_c24', 'string'),\n",
       " ('_c25', 'string'),\n",
       " ('_c26', 'string'),\n",
       " ('_c27', 'string'),\n",
       " ('_c28', 'string'),\n",
       " ('_c29', 'string'),\n",
       " ('_c30', 'string'),\n",
       " ('_c31', 'string'),\n",
       " ('_c32', 'string'),\n",
       " ('_c33', 'string'),\n",
       " ('_c34', 'string'),\n",
       " ('_c35', 'string'),\n",
       " ('_c36', 'string'),\n",
       " ('_c37', 'string'),\n",
       " ('_c38', 'string'),\n",
       " ('_c39', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_adds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, a continuación, cogemos únicamente aquellas que sean **int**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_column_list = [item[0] for item in dfs_adds.dtypes if item[1] == 'int']\n",
    "num_column_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y las metemos en un nuevo dataframe llamado **dfs_num**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----+---+---+---+---+----+----+----+----+\n",
      "|_c0|_c1|_c2|_c3|_c4| _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|\n",
      "+---+---+---+---+---+----+---+---+---+---+----+----+----+----+\n",
      "|  0|  1|  1|  5|  0|1382|  4| 15|  2|181|   1|   2|null|   2|\n",
      "+---+---+---+---+---+----+---+---+---+---+----+----+----+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_num =  dfs_adds.select(num_column_list)\n",
    "dfs_num.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5 ¿Cuántos valores nulos existen en cada columna? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para no tener que calcular en número de nulos de cada columna manualmente, hacemos un bucle para recorrer todas las columnas y así crear una lista que contendrá una \"*query*\" por columna que cuente sus nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'count(CASE WHEN (_c0 IS NULL) THEN _c0 END) AS `_c0`'>,\n",
       " Column<b'count(CASE WHEN (_c1 IS NULL) THEN _c1 END) AS `_c1`'>,\n",
       " Column<b'count(CASE WHEN (_c2 IS NULL) THEN _c2 END) AS `_c2`'>,\n",
       " Column<b'count(CASE WHEN (_c3 IS NULL) THEN _c3 END) AS `_c3`'>,\n",
       " Column<b'count(CASE WHEN (_c4 IS NULL) THEN _c4 END) AS `_c4`'>,\n",
       " Column<b'count(CASE WHEN (_c5 IS NULL) THEN _c5 END) AS `_c5`'>,\n",
       " Column<b'count(CASE WHEN (_c6 IS NULL) THEN _c6 END) AS `_c6`'>,\n",
       " Column<b'count(CASE WHEN (_c7 IS NULL) THEN _c7 END) AS `_c7`'>,\n",
       " Column<b'count(CASE WHEN (_c8 IS NULL) THEN _c8 END) AS `_c8`'>,\n",
       " Column<b'count(CASE WHEN (_c9 IS NULL) THEN _c9 END) AS `_c9`'>,\n",
       " Column<b'count(CASE WHEN (_c10 IS NULL) THEN _c10 END) AS `_c10`'>,\n",
       " Column<b'count(CASE WHEN (_c11 IS NULL) THEN _c11 END) AS `_c11`'>,\n",
       " Column<b'count(CASE WHEN (_c12 IS NULL) THEN _c12 END) AS `_c12`'>,\n",
       " Column<b'count(CASE WHEN (_c13 IS NULL) THEN _c13 END) AS `_c13`'>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counter_querys = [count(when(col(c).isNull(), c)).alias(c) for c in dfs_num.columns]\n",
    "null_counter_querys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de esto, se cuenta el número de nulos de cada columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+-------+-------+--------+-------+-----+-------+--------+-------+--------+-------+\n",
      "|_c0|     _c1|_c2|    _c3|    _c4|    _c5|     _c6|    _c7|  _c8|    _c9|    _c10|   _c11|    _c12|   _c13|\n",
      "+---+--------+---+-------+-------+-------+--------+-------+-----+-------+--------+-------+--------+-------+\n",
      "|  0|20793556|  0|9839447|9937369|1183117|10252328|1982866|22773|1982866|20793556|1982866|35071652|9937369|\n",
      "+---+--------+---+-------+-------+-------+--------+-------+-----+-------+--------+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_num.select(null_counter_querys).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6. ¿Cuál es la media de cada una de las columnas? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo que en el anterior ejercicio, pero esta vez para calcular la media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'round(avg(_c0), 4) AS `_c0`'>,\n",
       " Column<b'round(avg(_c1), 4) AS `_c1`'>,\n",
       " Column<b'round(avg(_c2), 4) AS `_c2`'>,\n",
       " Column<b'round(avg(_c3), 4) AS `_c3`'>,\n",
       " Column<b'round(avg(_c4), 4) AS `_c4`'>,\n",
       " Column<b'round(avg(_c5), 4) AS `_c5`'>,\n",
       " Column<b'round(avg(_c6), 4) AS `_c6`'>,\n",
       " Column<b'round(avg(_c7), 4) AS `_c7`'>,\n",
       " Column<b'round(avg(_c8), 4) AS `_c8`'>,\n",
       " Column<b'round(avg(_c9), 4) AS `_c9`'>,\n",
       " Column<b'round(avg(_c10), 4) AS `_c10`'>,\n",
       " Column<b'round(avg(_c11), 4) AS `_c11`'>,\n",
       " Column<b'round(avg(_c12), 4) AS `_c12`'>,\n",
       " Column<b'round(avg(_c13), 4) AS `_c13`'>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_querys = [round(mean(c), 4).alias(c) for c in dfs_num.columns]\n",
    "mean_querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+------+------+----------+--------+-------+------+--------+------+------+-----+------+\n",
      "|   _c0|   _c1|     _c2|   _c3|   _c4|       _c5|     _c6|    _c7|   _c8|     _c9|  _c10|  _c11| _c12|  _c13|\n",
      "+------+------+--------+------+------+----------+--------+-------+------+--------+------+------+-----+------+\n",
      "|0.2562|3.5024|105.8484|26.913|7.3227|18538.9917|116.0619|16.3331|12.517|106.1098|0.6175|2.7328|0.991|8.2175|\n",
      "+------+------+--------+------+------+----------+--------+-------+------+--------+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mean = dfs_num.select(mean_querys)\n",
    "df_mean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 7. Sustituye los valores de cada columna por la media **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sustituir los valores nulos de cada columna por la media, utilizaremos el dataframe **df_mean** del ejercicio anterior. A este dataframe le aplicaremos la función **collect()**, la cual nos devolverá sus valores en una estructura de datos de tipo *lista*. Esta operación es muy útil después de un filtro o cualquier otra operación que devuelva un subconjunto de datos suficientemente pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.2562, _c1=3.5024, _c2=105.8484, _c3=26.913, _c4=7.3227, _c5=18538.9917, _c6=116.0619, _c7=16.3331, _c8=12.517, _c9=106.1098, _c10=0.6175, _c11=2.7328, _c12=0.991, _c13=8.2175)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_list = df_mean.collect()\n",
    "mean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, convertimos la lista anterior a una estructura de datos de tipo *diccionario* de Python en el que las claves son los nombres de las columnas del dataframe. De esta forma, al aplicar la función **fillna**, para sustituir los valores nulos, se relacionarán ambas estructuras para aplicar la media a su respectiva columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dict = mean_list[0].asDict()\n",
    "dfs_num_no_na = dfs_num.fillna(mean_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Finalmente, comprobamos que ya no queden valores nulos en ninguna columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+----+----+----+----+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+----+----+----+\n",
      "|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   0|   0|   0|   0|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counter_querys_no_na = [count(when(col(c).isNull(), c)).alias(c) for c in dfs_num_no_na.columns]\n",
    "dfs_num_no_na.select(null_counter_querys_no_na).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y que la media de las columnas que tenían nulos haya cambiado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+-------+------+----------+-------+-------+-------+--------+------+------+------+------+\n",
      "|   _c0|   _c1|     _c2|    _c3|   _c4|       _c5|    _c6|    _c7|    _c8|     _c9|  _c10|  _c11|  _c12|  _c13|\n",
      "+------+------+--------+-------+------+----------+-------+-------+-------+--------+------+------+------+------+\n",
      "|0.2562|3.2745|105.8484|26.7171|7.2527|18538.9661|116.048|16.3187|12.5168|106.1051|0.3374|2.7011|0.2328|8.1703|\n",
      "+------+------+--------+-------+------+----------+-------+-------+-------+--------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_querys_no_na = [round(mean(c), 4).alias(c) for c in dfs_num_no_na.columns]\n",
    "dfs_num_no_na.select(mean_querys_no_na).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:green'>Bonus<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permite entrenar modelos de ML sobre grandes volúmenes de datos en paralelo. Échale un vistazo a la documentación [here](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#binomial-logistic-regression) e intenta crear un modelo de clasificación usando sólo las variables numéricas anteriores.\n",
    "\n",
    "Utiliza los últimos 10M de filas como set de testeo.\n",
    "\n",
    "La variable objetivo esta en la primera columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Crea un modelo de clasificación en Spark usando solo las variables numéricas. ¿Qué AUC ROC obtienes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero hay que separar los predictores (*features*) y la variable a predecir (*label*) de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+-----+\n",
      "|features                                                        |label|\n",
      "+----------------------------------------------------------------+-----+\n",
      "|[1.0,1.0,5.0,0.0,1382.0,4.0,15.0,2.0,181.0,1.0,2.0,0.0,2.0]     |0    |\n",
      "|[2.0,0.0,44.0,1.0,102.0,8.0,2.0,2.0,4.0,1.0,1.0,0.0,4.0]        |0    |\n",
      "|[2.0,0.0,1.0,14.0,767.0,89.0,4.0,2.0,245.0,1.0,3.0,3.0,45.0]    |0    |\n",
      "|(13,[0,1,2,3,4,5,12],[3.0,893.0,26.0,7.0,4392.0,116.0,8.0])     |0    |\n",
      "|(13,[0,1,2,4,6,9,10],[3.0,-1.0,26.0,2.0,3.0,1.0,1.0])           |0    |\n",
      "|[3.0,-1.0,26.0,7.0,12824.0,116.0,0.0,0.0,6.0,0.0,0.0,0.0,8.0]   |0    |\n",
      "|[3.0,1.0,2.0,7.0,3168.0,116.0,0.0,1.0,2.0,0.0,0.0,0.0,8.0]      |0    |\n",
      "|(13,[0,1,2,6,9,10],[1.0,4.0,2.0,1.0,1.0,1.0])                   |1    |\n",
      "|[3.0,44.0,4.0,8.0,19010.0,249.0,28.0,31.0,141.0,0.0,1.0,0.0,8.0]|0    |\n",
      "|[3.0,35.0,26.0,1.0,33737.0,21.0,1.0,2.0,3.0,0.0,1.0,0.0,1.0]    |0    |\n",
      "|[3.0,2.0,632.0,0.0,56770.0,116.0,0.0,5.0,65.0,0.0,0.0,0.0,2.0]  |0    |\n",
      "|[0.0,6.0,6.0,6.0,421.0,109.0,1.0,7.0,107.0,0.0,1.0,0.0,6.0]     |0    |\n",
      "|[0.0,-1.0,26.0,7.0,1465.0,0.0,17.0,0.0,4.0,0.0,4.0,0.0,8.0]     |1    |\n",
      "|[3.0,2.0,11.0,5.0,10262.0,34.0,2.0,4.0,5.0,0.0,1.0,0.0,5.0]     |1    |\n",
      "|[0.0,51.0,84.0,4.0,3633.0,26.0,1.0,4.0,8.0,0.0,1.0,0.0,4.0]     |0    |\n",
      "|[3.0,2.0,1.0,18.0,20255.0,116.0,0.0,1.0,1306.0,0.0,0.0,0.0,20.0]|0    |\n",
      "|[1.0,987.0,26.0,2.0,105.0,2.0,1.0,2.0,2.0,1.0,1.0,0.0,2.0]      |1    |\n",
      "|[0.0,1.0,26.0,0.0,16597.0,557.0,3.0,5.0,123.0,0.0,1.0,0.0,1.0]  |0    |\n",
      "|[0.0,24.0,4.0,2.0,2056.0,12.0,6.0,10.0,83.0,0.0,1.0,0.0,2.0]    |0    |\n",
      "|[7.0,102.0,26.0,3.0,780.0,15.0,7.0,15.0,15.0,1.0,1.0,0.0,3.0]   |0    |\n",
      "+----------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"_c1\", \"_c2\", \"_c3\", \"_c4\", \"_c5\", \"_c6\", \"_c7\", \"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dfs_num_no_na)\n",
    "df_ml = output.select(\"features\", \"_c0\").withColumnRenamed(\"_c0\", \"label\")\n",
    "\n",
    "df_ml.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "A continuación, separamos el conjunto de datos en datos de prueba y de entrenamiento. No hemos podido coger los últimos 10M de filas como conjunto de prueba porque no hemos encontrado un método eficiente en cuanto a tiempo y en cuanto a recursos de la máquina. Por esta razón, usamos la función **randomSplit**, que lo hace automáticamente y de una forma aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = df_ml.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Realizamos una regresión lineal sobre el conjunto de datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.00285541048927,0.000156361982228,0.000112500918501,0.0102146335971,-3.7697698062e-06,-0.00107398744422,-0.00184713706832,-0.0105201492498,9.86144925466e-05,0.428181704389,0.0531451879372,0.0281082807693,-0.0305578395883]\n",
      "Intercept: -0.9628918853089147\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Fit the model\n",
    "log_reg = lr.fit(training_data)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(log_reg.coefficients))\n",
    "print(\"Intercept: \" + str(log_reg.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13,[0,1,2,3,4,5,...|    0|[1.10461320147489...|[0.75112348229997...|       0.0|\n",
      "|(13,[0,1,2,3,4,5,...|    0|[1.29816167736331...|[0.78552543378549...|       0.0|\n",
      "|(13,[0,1,2,3,4,5,...|    0|[1.26221889061081...|[0.77940784094338...|       0.0|\n",
      "|(13,[0,1,2,3,4,5,...|    0|[1.36990236512496...|[0.79736437872795...|       0.0|\n",
      "|(13,[0,1,2,3,4,5,...|    1|[1.29290443248693...|[0.78463838878342...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = log_reg.transform(testing_data)\n",
    "\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Y, por último, calculamos el área bajo la curva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Area Under ROC: 0.6816807614157969\n",
      "Test: Area Under ROC: 0.6819624856937434\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "print(\"Training: Area Under ROC: \" + str(log_reg.summary.areaUnderROC))\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### La bibliografía usada para realizar este ejercicio de bonus ha sido la siguiente:\n",
    "\n",
    "* https://spark.apache.org/docs/2.2.0/ml-classification-regression.html\n",
    "\n",
    "* https://mapr.com/blog/churn-prediction-pyspark-using-mllib-and-ml-packages/\n",
    "\n",
    "* https://wesslen.github.io/twitter/predicting_twitter_profile_location_with_pyspark/\n",
    "\n",
    "* https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler\n",
    "\n",
    "* https://docs.databricks.com/spark/latest/mllib/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
